
# ğŸ§  TP â€“ Data Lake & Data Warehouse
### Projet : Ingestion, transformation et analyse de flux Kafka dans un Data Lake et Data Warehouse
**Auteur :** Jeff Gbanziali  
**Ã‰cole :** EFREI Paris â€“ Master 1 Data Engineering & IA  
**Date :** 31 octobre 2025  

---

## ğŸš€ Objectif du TP

Lâ€™objectif de ce TP est de construire une **chaÃ®ne complÃ¨te de traitement de donnÃ©es** reposant sur :
- **Kafka / Confluent Platform** pour lâ€™ingestion en streaming,  
- **un Data Lake** (stockage brut en JSONL),  
- **un Data Warehouse (SQLite)** pour les analyses,  
- et **une orchestration automatique** via un scheduler Python.

Ce TP simule un cas rÃ©el dâ€™ingÃ©nierie des donnÃ©es : ingestion, transformation, gouvernance et analyse continue des transactions.

---

## ğŸ—ï¸ Architecture Globale du Pipeline

```
Kafka / Confluent Platform
        â”‚
        â–¼
Python Consumer (confluent_kafka)
        â”‚
        â–¼
Data Lake (dossiers JSONL par topic / date)
        â”‚
        â–¼
SQLite Data Warehouse
        â”‚
        â–¼
RequÃªtes SQL dâ€™analyse & visualisation
```

---

## âš™ï¸ 1. PrÃ©paration de lâ€™Environnement Kafka

### ğŸ“¦ Ã‰tape 1 : DÃ©marrage de Confluent Kafka (TP1 cp-all-in-one)

> ğŸ’¡ Le professeur ou Ã©valuateur doit **dÃ©marrer le cluster Kafka** depuis le dossier **`cp-all-in-one`** fourni avec le TP1.

Dans un terminal Docker :
```bash
cd cp-all-in-one
docker-compose up -d
```

Cela lance :
- Zookeeper (port 2181)  
- Kafka Broker (9092)  
- Schema Registry (8081)  
- Kafka Connect (8083)  
- ksqlDB Server (8088)  
- Control Center (9021)  
- REST Proxy (8082)

VÃ©rifiez le bon dÃ©marrage :
```bash
docker ps
```

Puis ouvrez **Confluent Control Center** :  
ğŸ‘‰ [http://localhost:9021](http://localhost:9021)

---

## ğŸ“‚ 2. Structure du Projet

```
data_lake/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ consumers/
â”‚   â”œâ”€â”€ kafka_to_datalake.py
â”‚   â””â”€â”€ fake_kafka_producer.py
â”œâ”€â”€ cp-all-in-one/
â”‚   â””â”€â”€ docker-compose.yml
â”‚   â””â”€â”€ kafka_producer_transaction.py
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ sqlite_loader.py
â”‚   â”œâ”€â”€ cleanup.py
â”‚   â”œâ”€â”€ permissions_manager.py
â”‚   â””â”€â”€ scheduler.py
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ TRANSACTIONS_USD/
â”‚   â”œâ”€â”€ TRANSACTIONS_SECURE/
â”‚   â”œâ”€â”€ TRANSACTIONS_FAILED/
â”‚   â”œâ”€â”€ TRANSACTIONS_PENDING/
â”‚   â”œâ”€â”€ TRANSACTIONS_BLACKLISTED/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ run_queries.py
â”œâ”€â”€ data_warehouse.db
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§° 3. Installation & Configuration

### Ã‰tape 1 : CrÃ©er lâ€™environnement Python
```bash
py -3.11 -m venv venv
venv\Scripts\activate
```

### Ã‰tape 2 : Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### Ã‰tape 3 : Exemple de requirements.txt
```txt
confluent-kafka
pandas
schedule
sqlite3-binary
reportlab
```

---

## ğŸ›°ï¸ 4. Consommation des flux Kafka vers le Data Lake

### Ã‰tape 1 : VÃ©rifiez vos topics dans Kafka
Dans le Control Center (http://localhost:9021) â†’ rubrique **Topics**, assurez-vous que ces topics existent, 
ensuite lancer ce script :

```bash
python -m cp-all-in-one.kafka_producer_transaction
```
Ce script va envoyer des messages dans notre kafka sous cette forme :
```bash

{
  "TRANSACTION_ID": "TXN-bac5ffc7",
  "TIMESTAMP": "2025-10-13T16:38:28.047846Z",
  "USER_ID": "USER-4026",
  "USER_NAME": "Johnson",
  "PRODUCT_ID": "PROD-678",
  "AMOUNT": 339.47,
  "CURRENCY": "AUD",
  "TRANSACTION_TYPE": "payment",
  "STATUS": "pending",
  "LOCATION": {
    "CITY": "Guangzhou",
    "COUNTRY": "China"
  },
  "PAYMENT_METHOD": "apple_pay",
  "PRODUCT_CATEGORY": "books",
  "QUANTITY": 5,
  "SHIPPING_ADDRESS": {
    "STREET": "174 Main St",
    "ZIP": "31472",
    "CITY": "Guangzhou",
    "COUNTRY": "China"
  },
  "DEVICE_INFO": {
    "OS": "MacOS",
    "BROWSER": "Edge",
    "IP_ADDRESS": "4.217.181.91"
  },
  "CUSTOMER_RATING": 3,
  "DISCOUNT_CODE": null,
  "TAX_AMOUNT": 0
}

```

```
TRANSACTIONS_SECURE
TRANSACTIONS_USD
TRANSACTIONS_BLACKLISTED
TRANSACTIONS_FAILED
TRANSACTIONS_PENDING
TRANSACTIONS_PROCESSING
TRANSACTIONS_COMPLETED
USER_SPENDING_BY_TYPE
SPEND_LAST_FIVE_MIN_BY_TYPE
```

### Ã‰tape 2 : Lancer le consumer
```bash
python -m consumers.kafka_to_datalake
```

ğŸ“¡ Le script Ã©coute les topics et Ã©crit les donnÃ©es dans :
```
pipeline/<topic>/<date>/<topic>_<date>.jsonl
```

---

## ğŸ¦ 5. Chargement dans le Data Warehouse (SQLite)

### Ã‰tape 1 : Lancer le chargement
```bash
python -m jobs.sqlite_loader
```

ğŸ“¦ Les fichiers JSONL sont convertis en tables SQLite :
```
data_warehouse.db
â”œâ”€â”€ TRANSACTIONS_SECURE
â”œâ”€â”€ TRANSACTIONS_USD
â”œâ”€â”€ TRANSACTIONS_FAILED
â”œâ”€â”€ USER_SPENDING_BY_TYPE
â””â”€â”€ SPEND_LAST_FIVE_MIN_BY_TYPE
```

---

## â° 6. Orchestration & Gouvernance

### Scheduler automatique (toutes les 10 minutes)
```bash
python -m jobs.scheduler
```

Ce fichier orchestre :
- le chargement Data Lake â†’ SQLite  
- le nettoyage des anciens fichiers (> 7 jours)  
- la vÃ©rification des permissions

---

## ğŸ”’ 7. Gestion des Permissions et Nettoyage

- `permissions_manager.py` â†’ crÃ©e une table `permissions` dans SQLite pour chaque utilisateur.  
- `cleanup.py` â†’ supprime automatiquement les anciens fichiers dans `pipeline/` pour limiter lâ€™espace disque.

---

## ğŸ“Š 8. Analyse et RequÃªtes SQL

### MÃ©thode 1 â€” Terminal SQLite

```bash
sqlite3 data_warehouse.db
```

### Exemples dâ€™analyses :
```sql
-- Total dÃ©pensÃ© par type de transaction
SELECT json_extract(data, '$.TRANSACTION_TYPE') AS transaction_type,
       SUM(json_extract(data, '$.AMOUNT_USD')) AS total_spent_usd
FROM TRANSACTIONS_USD
GROUP BY transaction_type;

-- Classement des utilisateurs les plus dÃ©pensiers
SELECT json_extract(data, '$.USER_ID_HASHED') AS user,
       SUM(json_extract(data, '$.AMOUNT_USD')) AS total
FROM TRANSACTIONS_USD
GROUP BY user
ORDER BY total DESC
LIMIT 10;
```

### MÃ©thode 2 â€” Script Python
```bash
python -m analysis.run_queries
```

---

## ğŸ“… 9. Rapport Final

Le rapport PDF est gÃ©nÃ©rÃ© automatiquement : `TP_DataLake_Report.pdf`

Il contient :
- Lâ€™architecture du pipeline  
- La structure du projet  
- La gouvernance et le scheduler  
- Les requÃªtes dâ€™analyse SQL  

---

## ğŸ‘¨â€ğŸ’» Auteur

**Jeff Gbanziali**  
Ã‰tudiant M1 Data Engineering & Intelligence Artificielle  
ğŸ“ EFREI Paris  
ğŸ“… Octobre 2025  
