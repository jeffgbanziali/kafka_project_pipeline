# üß† TP ‚Äì Data Lake & Data Warehouse (Confluent + Kafka + SQLite)

## üéØ Objectif du TP
Ce projet met en place une **cha√Æne compl√®te de traitement de donn√©es en streaming et batch**, bas√©e sur **Kafka (Confluent Platform)**, un **Data Lake** (stockage local structur√©) et un **Data Warehouse** sous **SQLite**.

L‚Äôobjectif est de :  
- Cr√©er un pipeline de bout en bout depuis Kafka jusqu‚Äôau stockage analytique.  
- Appliquer les principes de l‚Äôing√©nierie des donn√©es : ingestion, transformation, stockage, et analyse.  
- Manipuler des flux de transactions s√©curis√©es, agr√©g√©es et horodat√©es.

---

## üèóÔ∏è Architecture g√©n√©rale

```text
Kafka Topics  ‚Üí  Data Lake (JSONL)  ‚Üí  SQLite Data Warehouse
   ‚Üë
   ‚îî‚îÄ‚îÄ KSQL Streams & Tables (transactions, status, spending, blacklist‚Ä¶)
```

### √âtapes principales :
1. **Ingestion Kafka :**
   - Utilisation de Confluent Platform (`docker-compose`) pour h√©berger Kafka, Schema Registry, KSQLDB, etc.
   - Topics cr√©√©s :  
     `TRANSACTIONS_SECURE`, `TRANSACTIONS_USD`, `TRANSACTIONS_BLACKLISTED`,  
     `TRANSACTIONS_COMPLETED`, `TRANSACTIONS_FAILED`, `TRANSACTIONS_PENDING`,  
     `TRANSACTIONS_PROCESSING`, `TRANSACTIONS_CANCELLED`,  
     `USER_SPENDING_BY_TYPE`, `SPEND_LAST_FIVE_MIN_BY_TYPE`.

2. **Data Lake :**
   - Stockage local des messages JSON dans :  
     `pipeline/<topic>/<date>/<topic>_<date>.jsonl`
   - Structuration journali√®re.
   - Enrichissement automatique via le consumer Kafka.

3. **Data Warehouse (SQLite) :**
   - Chargement automatique depuis le Data Lake.
   - Tables cr√©√©es automatiquement :  
     `TRANSACTIONS_SECURE`, `TRANSACTIONS_USD`, `USER_SPENDING_BY_TYPE`,  
     `SPEND_LAST_FIVE_MIN_BY_TYPE`, etc.
   - Chaque table contient :
     ```sql
     id INTEGER PRIMARY KEY,
     data JSON,
     imported_at TIMESTAMP
     ```

---

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Cloner le projet
```bash
git clone <repo-url>
cd data_lake
python -m venv venv
venv\Scripts\activate  # Windows
```

### 2Ô∏è‚É£ Installer les d√©pendances
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Lancer la stack Kafka Confluent
```bash
docker-compose up -d
```

### 4Ô∏è‚É£ D√©marrer le consumer Kafka ‚Üí Data Lake
```bash
python -m consumers.kafka_to_datalake
```

Le script √©coute tous les topics list√©s dans `config/settings.py` et √©crit les messages dans `pipeline/`.

### 5Ô∏è‚É£ Charger les donn√©es dans SQLite
```bash
python -m jobs.sqlite_loader
```

Une base `data_warehouse.db` est alors cr√©√©e/actualis√©e.

---

## üîÑ Pipeline de traitement

### **1. KSQL Streams**
Les streams cr√©√©s depuis `TRANSACTIONS_SECURE` :
```sql
CREATE STREAM TRANSACTIONS_USD AS
SELECT
  TRANSACTION_ID,
  USER_ID_HASHED,
  AMOUNT,
  CURRENCY,
  TRANSACTION_TYPE,
  STATUS,
  CASE
    WHEN CURRENCY = 'EUR' THEN AMOUNT * 1.1
    WHEN CURRENCY = 'GBP' THEN AMOUNT * 1.25
    WHEN CURRENCY = 'CAD' THEN AMOUNT * 0.74
    ELSE AMOUNT
  END AS AMOUNT_USD,
  'USD' AS TARGET_CURRENCY
FROM TRANSACTIONS_SECURE
EMIT CHANGES;
```

### **2. Tables analytiques**
Exemple : total d√©pens√© par utilisateur et par type :
```sql
CREATE TABLE USER_SPENDING_BY_TYPE AS
SELECT USER_ID_HASHED, TRANSACTION_TYPE,
       SUM(AMOUNT_USD) AS TOTAL_SPENT_USD
FROM TRANSACTIONS_USD
GROUP BY USER_ID_HASHED, TRANSACTION_TYPE
EMIT CHANGES;
```

### **3. Fen√™tres glissantes (5 minutes)**
```sql
CREATE TABLE SPEND_LAST_FIVE_MIN_BY_TYPE AS
SELECT TRANSACTION_TYPE,
       SUM(AMOUNT_USD) AS TOTAL_LAST5MIN
FROM TRANSACTIONS_USD
WINDOW HOPPING (SIZE 5 MINUTES, ADVANCE BY 1 MINUTE)
GROUP BY TRANSACTION_TYPE
EMIT CHANGES;
```

---

## üß© Structure du projet

```text
data_lake/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Configuration g√©n√©rale (Kafka, Data Lake, Topics)
‚îú‚îÄ‚îÄ consumers/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_to_datalake.py     # Consumer Kafka ‚Üí Data Lake
‚îÇ   ‚îî‚îÄ‚îÄ fake_kafka_producer.py   # G√©n√©rateur de donn√©es de test
‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îî‚îÄ‚îÄ sqlite_loader.py         # Charge le Data Lake ‚Üí SQLite
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ TRANSACTIONS_SECURE/
‚îÇ   ‚îú‚îÄ‚îÄ TRANSACTIONS_USD/
‚îÇ   ‚îú‚îÄ‚îÄ TRANSACTIONS_BLACKLISTED/
‚îÇ   ‚îú‚îÄ‚îÄ USER_SPENDING_BY_TYPE/
‚îÇ   ‚îî‚îÄ‚îÄ SPEND_LAST_FIVE_MIN_BY_TYPE/
‚îî‚îÄ‚îÄ data_warehouse.db            # Base SQLite g√©n√©r√©e automatiquement
```

---

## üßÆ Analyse dans SQLite

### Lister les tables
```sql
.tables
```

### Voir les premi√®res lignes
```sql
SELECT * FROM TRANSACTIONS_USD LIMIT 5;
```

### Extraire des champs JSON
```sql
SELECT
  json_extract(data, '$.TRANSACTION_ID') AS TRANSACTION_ID,
  json_extract(data, '$.AMOUNT_USD') AS AMOUNT_USD,
  json_extract(data, '$.TRANSACTION_TYPE') AS TRANSACTION_TYPE
FROM TRANSACTIONS_USD
LIMIT 10;
```

### Agr√©ger par type
```sql
SELECT
  json_extract(data, '$.TRANSACTION_TYPE') AS type,
  SUM(json_extract(data, '$.AMOUNT_USD')) AS total
FROM TRANSACTIONS_USD
GROUP BY type;
```

---

## üß† Points d‚Äôapprentissage

| Th√®me | Comp√©tence acquise |
|-------|--------------------|
| Kafka / Confluent | Cr√©ation de topics, production et consommation de flux |
| KSQLDB | Manipulation de donn√©es en streaming et agr√©gation temps r√©el |
| Data Lake | Stockage brut JSON structur√© par date et sujet |
| Data Warehouse | Int√©gration analytique (SQLite) |
| Python | Automatisation du pipeline (producers, consumers, loaders) |

---

## üèÅ R√©sultat final

‚úÖ Un pipeline de donn√©es complet :  
Kafka ‚Üí KSQL ‚Üí Data Lake (JSON) ‚Üí SQLite (analytique).

‚úÖ Une architecture conforme √† un **mini-lab d‚Äôing√©nierie de donn√©es moderne**.  
‚úÖ R√©utilisable pour d‚Äôautres sujets (IoT, transactions bancaires, logs web, etc.).

---

üìÖ Date de r√©alisation : **31/10/2025**  
üë®‚Äçüíª Auteur : **Jeff Gbanziali**
